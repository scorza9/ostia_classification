{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Import Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# install required libraries\n","!pip install git+https://github.com/AAMIASoftwares-research/HCATNetwork.git@google-colab\n","!pip install git+https://github.com/AAMIASoftwares-research/DatasetUtilities.git@google-colab"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# import general libraries\n","import os\n","import sys\n","import subprocess\n","from IPython.display import FileLink\n","import pickle\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import time\n","from typing import List\n","# import data libraries\n","import hcatnetwork\n","import HearticDatasetManager"]},{"cell_type":"markdown","metadata":{},"source":["# Define Global Variables and Utilities"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Set folder paths\n","ASOCA_FOLDER = \"/kaggle/input/heart-data/ASOCA/ASOCA\"\n","CAT08_FOLDER = \"/kaggle/input/heart-data/CAT08/CAT08\""]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# global variables\n","NUM_PATIENTS_ASOCA = 40\n","NUM_PATIENTS_CAT08 = 8\n","CUBE_SIDE_MM = 12\n","CUBE_ISOTROPIC_SPACING_MM = 0.5\n","CUBE_SIDE_N_SAMPLES = int(CUBE_SIDE_MM * (1/CUBE_ISOTROPIC_SPACING_MM))\n","NUM_CUBES_PATIENT = 100"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# define workspace utility functions\n","\n","# call this function when in need to free RAM space\n","def check_variables():\n","    \"\"\"\n","    Check the memory usage of variables in the workspace.\n","    Print variables and their memory sizes in descending order.\n","    \"\"\"\n","    # get the memory size of each variable\n","    variable_sizes = {k: sys.getsizeof(v) for k, v in locals().items() if not k.startswith('__')}\n","    # sort the variables based on their memory size\n","    sorted_variables = sorted(variable_sizes.items(), key=lambda x: x[1], reverse=True)\n","    # print the variables and their memory sizes in descending order\n","    for var, size in sorted_variables:\n","        print(f\"{var}: {size} bytes\")\n","\n","# save anything via pickle\n","def save(item, name: str, path=\"/kaggle/working/\"):\n","    \"\"\"\n","    Save an item using pickle.\n","\n","    Parameters:\n","        item: The item to be saved.\n","        name (str): The name of the file.\n","        path (str): The path where the file will be saved (default: \"/kaggle/working/\").\n","    \"\"\"\n","    item_file = path + name\n","    with open(item_file, 'wb') as file:\n","        pickle.dump(item, file)\n","\n","# download item as zip\n","def download_file(source_path: str, download_file_name: str, output_path=\"/kaggle/working/\"):\n","    \"\"\"\n","    Create a zip file from the specified source path and provide a download link.\n","    \n","    Parameters:\n","        source_path (str): The path to the source file or directory to be zipped.\n","        download_file_name (str): The name of the zip file and download link.\n","        output_path (str): The output path for the zip file (default: \"/kaggle/working/\").\n","    \"\"\"\n","    # save the current working directory\n","    current_working_directory = os.getcwd()  \n","    os.chdir(output_path)\n","\n","    try:\n","        zip_name = f\"{download_file_name}.zip\"\n","        command = f\"zip {zip_name} {source_path} -r\"\n","        result = subprocess.run(command, shell=True, capture_output=True, text=True)\n","        if result.returncode != 0:\n","            raise RuntimeError(f\"Unable to run zip command! Error: {result.stderr}\")\n","\n","        display(FileLink(zip_name))\n","    finally:\n","        # restore the original working directory\n","        os.chdir(current_working_directory)  "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# define graphs utility functions \n","\n","# compute maximum, minimum and average radius for graphs nodes\n","def compute_r_stats(graph: hcatnetwork.graph.graph.SimpleCenterlineGraph):\n","    \"\"\"\n","    Given a SimpleCenterlineGraph, calculate statistics on node radii.\n","\n","    Parameters:\n","        graph (SimpleCenterlineGraph): The graph containing node radii.\n","\n","    Returns:\n","        tuple: A tuple containing the maximum, minimum, and average radius.\n","    \"\"\"\n","    minR = 10\n","    maxR = 0\n","    avgR = 0\n","    for node_id in graph.nodes:\n","        r = graph.nodes[node_id][\"r\"]\n","        avgR += r\n","        if(r>maxR): maxR=r\n","        elif(r<minR): minR = r\n","\n","    avgR /= len(graph.nodes)\n","    \n","    return maxR, minR, avgR\n","\n","# save for each pair of Ostia the corresponding coordinates and radius\n","def get_ostia_data(graph: hcatnetwork.graph.graph.SimpleCenterlineGraph):\n","    \"\"\"\n","    For a given SimpleCenterlineGraph, get data for pairs of Ostia nodes.\n","\n","    Parameters:\n","        graph (SimpleCenterlineGraph): The graph containing coronary Ostia nodes.\n","\n","    Returns:\n","        tuple: A tuple containing node IDs, coordinates, and radii for two Ostia nodes.\n","    \"\"\"\n","    ids = graph.get_coronary_ostia_node_id()\n","    coord_ostium_1 = np.array([graph.nodes[ids[0]]['x'],graph.nodes[ids[0]]['y'],graph.nodes[ids[0]]['z'],\n","                               np.array([graph.nodes[ids[0]]['r']])])\n","    coord_ostium_2 = np.array([graph.nodes[ids[1]]['x'],graph.nodes[ids[1]]['y'],graph.nodes[ids[1]]['z'],\n","                               np.array([graph.nodes[ids[1]]['r']])])\n","    return ids, coord_ostium_1, coord_ostium_2\n","\n","# generates coordinate matrices from x,y,z sampled points. Flattening and trasposing is then performed to generate (Nx3) arrays\n","def get_cube_sample_points(center: np.ndarray, side_mm: float, n_samples_per_side: int):\n","    \"\"\"\n","    Sample a cube centered at the given point with a specified number of points per side.\n","\n","    Parameters:\n","        center (np.ndarray): The center coordinates of the cube.\n","        side_mm (float): The side length of the cube in millimeters.\n","        n_samples_per_side (int): The number of sample points per side.\n","\n","    Returns:\n","        np.ndarray: A (Nx3) array containing the sampled points.\n","    \"\"\"\n","    xs = np.linspace(center[0] - side_mm/2, center[0] + side_mm/2, n_samples_per_side)\n","    ys = np.linspace(center[1] - side_mm/2, center[1] + side_mm/2, n_samples_per_side)\n","    zs = np.linspace(center[2] - side_mm/2, center[2] + side_mm/2, n_samples_per_side)\n","    return np.array(np.meshgrid(xs, ys, zs)).reshape(3, -1).T\n","\n","# returns samples as a (NxNxN) array\n","def cube_samples_to_array(samples: np.ndarray, n_samples_per_side: int) -> np.ndarray:\n","    \"\"\"\n","    Convert sampled points from a cube to a 3D numpy array.\n","\n","    Parameters:\n","        samples (np.ndarray): The sampled points.\n","        n_samples_per_side (int): The number of samples per side.\n","\n","    Returns:\n","        np.ndarray: A 3D array representing the cube.\n","    \"\"\"\n","    return samples.reshape(n_samples_per_side, n_samples_per_side, n_samples_per_side)\n","\n","# returns directly the cube as numpy (NxNxN) array from center point in RAS coordinates\n","def get_input_data_from_vertex_ras_position(\n","        image: HearticDatasetManager.cat08.Cat08ImageCT|HearticDatasetManager.asoca.AsocaImageCT,\n","        position: np.ndarray,\n","        side_mm: float,\n","        n_samples_per_side: int,\n","        affine=np.eye(4)\n","    ) -> np.ndarray:\n","    \"\"\"Get the input data from a vertex position expressed in RAS coordinates system.\n","\n","    Parameters\n","    ----------\n","    image : HearticDatasetManager.cat08.Cat08ImageCT | HearticDatasetManager.asoca.AsocaImageCT\n","        The image from which to extract the data.\n","    position : numpy.ndarray\n","        The position of the cube center in RAS coordinates system.\n","    side_mm : float\n","        The side of the cube in mm.\n","    n_samples_per_side : int\n","        The number of samples per side.\n","    affine : np.ndarray, optional\n","        The affine transformation to apply to the position of the samples used to create the cube, by default np.eye(4) (which does nothing).\n","        This is useful in data augmemtation, if you want to rotate, flip, or do whatever operation\n","        on the cube sample points, you can do it by passing the affine transformation here.\n","        For example, HearticDatasetManager.affine.get_affine_3d_rotation_around_vector() will rotate the cube (see the function docs).\n","    \"\"\"\n","    # Get the cube sample points\n","    cube_pos = get_cube_sample_points(position, side_mm, n_samples_per_side)\n","    # Apply transformation affine if any\n","    if affine is None:\n","        affine = np.eye(4)\n","    cube_pos = HearticDatasetManager.affine.apply_affine_3d(affine, cube_pos)\n","    # Sample the image\n","    samples = image.sample(cube_pos, interpolation=\"linear\").T\n","    # Convert to ndarray\n","    cube_array = cube_samples_to_array(samples, n_samples_per_side)\n","    return cube_array"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# define functions to load ASOCA and CAT08 data\n","\n","# ASOCA\n","def load_ASOCA_data():\n","    \"\"\"\n","    Load all patients' images and graphs for ASOCA dataset.\n","\n","    Returns:\n","        tuple: A tuple containing lists of ASOCA images and graphs.\n","    \"\"\"\n","    images = []\n","    graphs = []\n","\n","    for i in tqdm(range(NUM_PATIENTS_ASOCA), desc=\"Loading ASOCA data\"):\n","        # Load image and graph\n","        if (i < NUM_PATIENTS_ASOCA / 2):\n","            # Normal patients\n","            image_file = os.path.join(\n","                ASOCA_FOLDER,\n","                HearticDatasetManager.asoca.DATASET_ASOCA_IMAGES_DICT[\"Normal\"][i]\n","            )\n","\n","            graph_file = os.path.join(\n","                ASOCA_FOLDER,\n","                HearticDatasetManager.asoca.DATASET_ASOCA_GRAPHS_RESAMPLED_05MM_DICT[\"Normal\"][i]\n","            )\n","        else:\n","            # Diseased patients\n","            image_file = os.path.join(\n","                ASOCA_FOLDER,\n","                HearticDatasetManager.asoca.DATASET_ASOCA_IMAGES_DICT[\"Diseased\"][i - NUM_PATIENTS_ASOCA // 2]\n","            )\n","\n","            graph_file = os.path.join(\n","                ASOCA_FOLDER,\n","                HearticDatasetManager.asoca.DATASET_ASOCA_GRAPHS_RESAMPLED_05MM_DICT[\"Diseased\"][i - NUM_PATIENTS_ASOCA // 2]\n","            )\n","\n","        image = HearticDatasetManager.asoca.AsocaImageCT(image_file)\n","        graph = hcatnetwork.io.load_graph(\n","            graph_file,\n","            output_type=hcatnetwork.graph.SimpleCenterlineGraph\n","        )\n","        # Convert graph coordinates to RAS\n","        for node_id in graph.nodes:\n","            old_coords = np.array(\n","                [graph.nodes[node_id][\"x\"], graph.nodes[node_id][\"y\"], graph.nodes[node_id][\"z\"]]\n","            )\n","            new_coords = HearticDatasetManager.affine.apply_affine_3d(image.affine_centerlines2ras, old_coords)\n","            graph.nodes[node_id][\"x\"] = new_coords[0]\n","            graph.nodes[node_id][\"y\"] = new_coords[1]\n","            graph.nodes[node_id][\"z\"] = new_coords[2]\n","\n","        images.append(image)\n","        graphs.append(graph)\n","\n","        # Simulating loading time for each iteration\n","        time.sleep(0.1)\n","\n","    return images, graphs\n","\n","# CAT08\n","def load_CAT08_data():\n","    \"\"\"\n","    Load all patients' images and graphs for CAT08 dataset.\n","\n","    Returns:\n","        tuple: A tuple containing lists of CAT08 images and graphs.\n","    \"\"\"\n","    images = []\n","    graphs = []\n","\n","    for i in tqdm(range(NUM_PATIENTS_CAT08), desc=\"Loading CAT08 data\"):\n","        # Load image and graph\n","        image_file = os.path.join(\n","            CAT08_FOLDER,\n","            HearticDatasetManager.cat08.DATASET_CAT08_IMAGES[i]\n","        )\n","\n","        graph_file = os.path.join(\n","            CAT08_FOLDER,\n","            HearticDatasetManager.cat08.DATASET_CAT08_GRAPHS_RESAMPLED_05MM[i]\n","        )\n","\n","        image = HearticDatasetManager.cat08.Cat08ImageCT(image_file)\n","        graph = hcatnetwork.io.load_graph(\n","            graph_file,\n","            output_type=hcatnetwork.graph.SimpleCenterlineGraph\n","        )\n","\n","        # Convert graph coordinates to RAS\n","        for node_id in graph.nodes:\n","            old_coords = np.array(\n","                [graph.nodes[node_id][\"x\"], graph.nodes[node_id][\"y\"], graph.nodes[node_id][\"z\"]]\n","            )\n","            new_coords = HearticDatasetManager.affine.apply_affine_3d(image.affine_centerlines2ras, old_coords)\n","            graph.nodes[node_id][\"x\"] = new_coords[0]\n","            graph.nodes[node_id][\"y\"] = new_coords[1]\n","            graph.nodes[node_id][\"z\"] = new_coords[2]\n","\n","        images.append(image)\n","        graphs.append(graph)\n","\n","        # Simulating loading time for each iteration\n","        time.sleep(0.1)\n","\n","    return images, graphs"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# define functions to extract volume patches from each patient's CT scan and the corresponding labels\n","\n","# returns the data representing the input to the network\n","def get_labeled_patches(\n","    image: HearticDatasetManager.cat08.Cat08ImageCT|HearticDatasetManager.asoca.AsocaImageCT,\n","    graph: hcatnetwork.graph.graph.SimpleCenterlineGraph,\n","    rotation_random_sampling = None,\n","    rotation_local_sampling = None\n","):\n","    \n","    \"\"\" \n","    The function takes as input a single image-graph pair and provides the extracted patches and labels by following these steps:\n","    \n","        1. Firstly it samples a point along the graph and applies a random translation to it.\n","        2. Then utility funcitons are used to sample a cube centered in such point.\n","        3. If the point lies close enough to one of the two Ostia, then label = 1, 0 otherwise.\n","        4. Finally, steps 1-2 are repeated by local sampling around the Ostium to ensure class 1 is represented. \n","    \n","    Parameters:\n","        image (HearticDatasetManager.cat08.Cat08ImageCT | HearticDatasetManager.asoca.AsocaImageCT):\n","            The CT scan image data.\n","        graph (hcatnetwork.graph.graph.SimpleCenterlineGraph):\n","            The corresponding centerline graph.\n","        rotation_random_sampling (optional): Rotation parameters for random sampling. Default is None.\n","        rotation_local_sampling (optional): Rotation parameters for local sampling. Default is None.\n","\n","    Returns:\n","        tuple: A tuple containing numpy arrays of extracted cubes and corresponding labels.\n","    \"\"\"\n","    \n","    # compute radius statistics\n","    maxR, minR, avgR = compute_r_stats(graph)\n","    # retrieve ostia\n","    ids, coord_ostium_1, coord_ostium_2 = get_ostia_data(graph)\n","    \n","    cubes = []\n","    labels = []\n","\n","    # firstly random sample along the graph\n","    for cube_i in tqdm(range(NUM_CUBES_PATIENT // 2), desc=\"Random Sampling\"):\n","        # choose a random node of the graph\n","        node_id = np.random.choice(list(graph.nodes.keys()))\n","        # get the position of the node in RAS\n","        node_position = np.array([graph.nodes[node_id][\"x\"], graph.nodes[node_id][\"y\"], graph.nodes[node_id][\"z\"]])\n","        # select parameters for random translation\n","        r = np.random.uniform(-avgR*1.5, avgR*1.5)#translation vector\n","        theta = np.random.uniform(0, 2*np.pi)#xy plane angle\n","        phi = np.random.uniform(0, np.pi)#z to xy plane angle\n","        # apply the translation to the selected point\n","        node_position += np.array([r*np.sin(phi)*np.cos(theta), r*np.sin(phi)*np.sin(theta), r*np.cos(phi)]).reshape(3,1)\n","        if (rotation_random_sampling):\n","            # define rotation vector\n","            vector_axis_of_rotation = np.array([np.random.uniform(-1, 1) , np.random.uniform(-1, 1), np.random.uniform(-1, 1)])\n","            transformation_to_apply = HearticDatasetManager.affine.get_affine_3d_rotation_around_vector(\n","                vector=vector_axis_of_rotation,\n","                vector_source=node_position.reshape(3,1), # the center of the cube\n","                rotation=np.random.choice(range(90+1)), # max degree of rotation +1 \n","                rotation_units=\"deg\"\n","            )\n","        else:\n","            transformation_to_apply = None\n","        # sample\n","        cube_array = get_input_data_from_vertex_ras_position(image,node_position,\n","                                                             CUBE_SIDE_MM,CUBE_SIDE_N_SAMPLES, \n","                                                             affine=transformation_to_apply)\n","        # compute distances from ostia to assign labels\n","        dist1 = np.linalg.norm(coord_ostium_1[:3] - node_position)\n","        dist2 = np.linalg.norm(coord_ostium_2[:3] - node_position)\n","        label = 1 if (dist1<=1.2*coord_ostium_1[-1] or dist2<=1.2*coord_ostium_2[-1]) else 0\n","\n","        cubes.append(cube_array)\n","        labels.append(label)\n","        # Simulating loading time for each iteration\n","        time.sleep(0.1)\n","        \n","    # then sample locally from the ostia applying augmentation via rotation if selected\n","    for cube_i in tqdm(range(NUM_CUBES_PATIENT // 2), desc=\"Local Sampling\"):\n","        # select one of the two ostia\n","        node_id = np.random.choice(ids)\n","        # get the position of the node in RAS\n","        node_position = np.array([graph.nodes[node_id][\"x\"], graph.nodes[node_id][\"y\"], graph.nodes[node_id][\"z\"]])\n","        # select parameters for random translation\n","        r = np.random.uniform(-graph.nodes[node_id][\"r\"]*1.2, graph.nodes[node_id][\"r\"]*1.2)#translation vector\n","        theta = np.random.uniform(0, 2*np.pi)#xy plane angle\n","        phi = np.random.uniform(0, np.pi)#z to xy plane angle\n","        # apply the translation to the selected point\n","        node_position += np.array([r*np.sin(phi)*np.cos(theta), r*np.sin(phi)*np.sin(theta), r*np.cos(phi)]).reshape(3,1)\n","        if (rotation_local_sampling):\n","            # define rotation vector\n","            vector_axis_of_rotation = np.array([np.random.uniform(-1, 1) , np.random.uniform(-1, 1), np.random.uniform(-1, 1)])\n","            transformation_to_apply = HearticDatasetManager.affine.get_affine_3d_rotation_around_vector(\n","                vector=vector_axis_of_rotation,\n","                vector_source=node_position.reshape(3,1), # the center of the cube\n","                rotation=np.random.choice(range(90+1)), # max degree of rotation +1 \n","                rotation_units=\"deg\"\n","            )\n","        else:\n","            transformation_to_apply = None\n","        # sample\n","        cube_array = get_input_data_from_vertex_ras_position(image,node_position,\n","                                                             CUBE_SIDE_MM,CUBE_SIDE_N_SAMPLES, \n","                                                             affine=transformation_to_apply)\n","\n","        cubes.append(cube_array)\n","        labels.append(1)\n","\n","        # Simulating loading time for each iteration\n","        time.sleep(0.1)\n","    \n","    \n","    cubes = np.array(cubes)\n","    labels = np.array(labels)\n","        \n","    return cubes, labels"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# define patient class able to store cubes and corresponding labels\n","class Patient:\n","    def __init__(self, cubes=None, labels=None):\n","        \"\"\"\n","        Initialize a Patient instance.\n","\n","        Parameters:\n","            cubes (List): List of cube data.\n","            labels (List): List of corresponding labels.\n","        \"\"\"\n","        self.cubes = cubes if cubes is not None else []\n","        self.labels = labels if labels is not None else []"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# define function to compute and visualize label distribution\n","def check_label_distribution(patients: List[Patient]):\n","    \"\"\"\n","    Compute and visualize the label distribution for a list of patients.\n","\n","    Parameters:\n","        patients (List[Patient]): List of Patient instances.\n","    \"\"\"\n","    # extract labels\n","    labels_distribution = [patient.labels for patient in patients]\n","    # flatten the list of lists into a single list\n","    all_labels = [label for sublist in labels_distribution for label in sublist]\n","\n","    # calculate counts\n","    label_counts = {0: all_labels.count(0), 1: all_labels.count(1)}\n","    # print counts\n","    for label, count in label_counts.items():\n","        print(f\"Label {label}: {count} occurrences\")\n","\n","    # plot the label distribution\n","    plt.hist(all_labels, bins=[0, 1, 2], align='left', rwidth=0.8)\n","    plt.xlabel('Label')\n","    plt.ylabel('Count')\n","    plt.title('Label Distribution')\n","    plt.xticks([0, 1])\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Data Loading and Volume Extraction"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# load data\n","print(\"--- Data Loading: ---\")\n","images_asoca, graphs_asoca = load_ASOCA_data()\n","images_cat08, graphs_cat08 = load_CAT08_data()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# perform train-validation-test splitting\n","# import train-test-split utility\n","from sklearn.model_selection import train_test_split\n","\n","# split patients to ensure that within the same set the data is of the same patient\n","def split_patients(patients: List[Patient], validation_size=0.1, test_size=0.1, random_seed=42):\n","    \"\"\"\n","    Split a list of Patient instances into train, validation, and test sets while keeping data for each patient together.\n","\n","    Parameters:\n","        patients (list): List of Patient instances.\n","        validation_size (float): Percentage of data to include in the validation set.\n","        test_size (float): Percentage of data to include in the test set.\n","        random_seed (int or None): Seed for reproducibility.\n","\n","    Returns:\n","        tuple: Three tuples (train_set, val_set, test_set), where each set is a list of Patient instances.\n","    \"\"\"\n","    # split patients into train+validation and test\n","    patients_train_val, patients_test = train_test_split(patients, test_size=test_size, random_state=random_seed)\n","\n","    # split the remaining patients into train and validation\n","    patients_train, patients_val = train_test_split(patients_train_val, test_size=validation_size, random_state=random_seed)\n","    \n","    return patients_train, patients_val, patients_test\n","\n","# extract X (cubes) and y (labels) for each set\n","def get_train_val_test(patients_train: List[Patient], patients_val: List[Patient], patients_test: List[Patient]):\n","    \"\"\"\n","    Extract X (cubes) and y (labels) for each set.\n","\n","    Parameters:\n","        patients_train (list): List of train set Patient instances.\n","        patients_val (list): List of validation set Patient instances.\n","        patients_test (list): List of test set Patient instances.\n","        \n","    Returns:\n","        tuple: Six numpy arrays (X_train, y_train, X_val, y_val, X_test, y_test) representing\n","               the features and labels for the training, validation, and test sets.\n","    \"\"\"\n","    \n","    # split data within patient sets\n","    X_train = np.array([patient.cubes for patient in patients_train])\n","    y_train = np.array([patient.labels for patient in patients_train])\n","\n","    X_val = np.array([patient.cubes for patient in patients_val])\n","    y_val = np.array([patient.labels for patient in patients_val])\n","\n","    X_test = np.array([patient.cubes for patient in patients_test])\n","    y_test = np.array([patient.labels for patient in patients_test])\n","\n","    return X_train, y_train, X_val, y_val, X_test, y_test"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# create empty Patient instances\n","patient_instances = []\n","for i in range(NUM_PATIENTS_ASOCA+NUM_PATIENTS_CAT08):\n","    patient_instance = Patient()\n","    patient_instances.append(patient_instance)\n","# split patients into train - validation - test\n","validation_size = 5\n","test_size = NUM_PATIENTS_CAT08\n","patients_train, patients_val, patients_test = split_patients(patient_instances, validation_size=validation_size, test_size=test_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# extract volumes: ASOCA is used to retrieve train and validation samples, while CAT08 is used entirely for testing \n","\n","# shuffle indexes for ASOCA volumes extraction\n","common_seed = 42\n","asoca_indexes = np.arange(NUM_PATIENTS_ASOCA)\n","np.random.seed(common_seed)\n","np.random.shuffle(asoca_indexes)\n","\n","print(\"--- ASOCA Volumes Extraction: ---\")\n","for i,patient_id in enumerate(asoca_indexes[:NUM_PATIENTS_ASOCA-validation_size]):\n","     # call the function to get labeled patches for the current patient\n","    print(f\"-> Patient {patient_id +1}:\")\n","    patients_train[i].cubes, patients_train[i].labels = get_labeled_patches(images_asoca[patient_id],\n","                                                                            graphs_asoca[patient_id],\n","                                                                            rotation_random_sampling=True,\n","                                                                            rotation_local_sampling=True\n","                                                                           )\n","\n","for i,patient_id in enumerate(asoca_indexes[NUM_PATIENTS_ASOCA-validation_size:]):\n","     # call the function to get labeled patches for the current patient\n","    print(f\"-> Patient {patient_id +1}:\")\n","    patients_val[i].cubes, patients_val[i].labels = get_labeled_patches(images_asoca[patient_id],\n","                                                                        graphs_asoca[patient_id], \n","                                                                        rotation_random_sampling=False,\n","                                                                        rotation_local_sampling=False\n","                                                                       )\n","\n","print(\"--- CAT08 Volumes Extraction: ---\")\n","# iterate over each CAT08 patient\n","for patient_id in range(NUM_PATIENTS_CAT08):\n","    # call the function to get labeled patches for the current patient\n","    print(f\"-> Patient {patient_id +1}:\")\n","    patients_test[patient_id].cubes, patients_test[patient_id].labels = get_labeled_patches(images_cat08[patient_id],\n","                                                                                            graphs_cat08[patient_id], \n","                                                                                            rotation_random_sampling=False,\n","                                                                                            rotation_local_sampling=False\n","                                                                                           )"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# check class proportion in train set\n","check_label_distribution(patients_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# check class proportion in validation set\n","check_label_distribution(patients_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# check class proportion in test set\n","check_label_distribution(patients_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# delete data to save RAM space\n","del images_asoca, graphs_asoca\n","del images_cat08, graphs_cat08"]},{"cell_type":"markdown","metadata":{},"source":["# Data Preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# apply clipping and standardization to the cubes\n","def preprocess_cubes(cubes: np.array, lower_clip=-800, upper_clip=1000):\n","    \"\"\"\n","    Preprocesses a set of 3D cubes by applying clipping and standardization.\n","\n","    Parameters:\n","    - cubes (np.array): An array of 3D cubes with shape (num_cubes, cube_size, cube_size, cube_size).\n","    - lower_clip (float): The lower bound for clipping values. Values below this bound will be set to this value.\n","    - upper_clip (float): The upper bound for clipping values. Values above this bound will be set to this value.\n","\n","    Returns:\n","    - np.array: An array of preprocessed cubes with the same shape as the input.\n","\n","    Note: The function does not modify the original 'cubes' array; it returns a new array with the processed cubes.\n","    \"\"\"\n","    # create an empty list to store preprocessed cubes\n","    preprocessed_cubes = []\n","    # iterate over each cube in the input array\n","    for cube in cubes:\n","        # clip values in the cube\n","        cube = np.clip(cube, lower_clip, upper_clip)\n","        # standardize values in the cube\n","        mean = np.mean(cube)\n","        std = np.std(cube)\n","        cube = (cube - mean) / std if std != 0 else (cube - mean)\n","        # append the preprocessed cube to the list\n","        preprocessed_cubes.append(cube)\n","    # convert the list of preprocessed cubes to a NumPy array and return\n","    return np.array(preprocessed_cubes)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# get split data\n","X_train, y_train, X_val, y_val, X_test, y_test = get_train_val_test(patients_train, patients_val, patients_test)\n","\n","# reshape data \n","input_shape = (24,24,24)\n","X_train = X_train.reshape((-1,) + input_shape)\n","X_val = X_val.reshape((-1,) + input_shape)\n","X_test = X_test.reshape((-1,) + input_shape)\n","\n","y_train = y_train.reshape(-1)\n","y_val = y_val.reshape(-1)\n","y_test = y_test.reshape(-1)\n","\n","# shuffle data\n","common_seed = 42\n","np.random.seed(common_seed)\n","\n","train_indices = np.arange(len(X_train))\n","np.random.shuffle(train_indices)\n","X_train = X_train[train_indices]\n","y_train = y_train[train_indices]\n","\n","val_indices = np.arange(len(X_val))\n","np.random.shuffle(val_indices)\n","X_val = X_val[val_indices]\n","y_val = y_val[val_indices]\n","\n","test_indices = np.arange(len(X_test))\n","np.random.shuffle(test_indices)\n","X_test = X_test[test_indices]\n","y_test = y_test[test_indices]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# check for data leaks\n","def check_data_leaks(X_train: np.array, X_val: np.array, X_test: np.array):\n","    \"\"\"\n","    Check for data leaks by comparing cubes between training, validation, and test sets.\n","\n","    Parameters:\n","        X_train (np.array): Cubes in the training set.\n","        X_val (np.array): Cubes in the validation set.\n","        X_test (np.array): Cubes in the test set.\n","    \"\"\"\n","    #check for duplicates in train and validation set\n","    for i,train_cube in enumerate(X_train):\n","        for j,val_cube in enumerate(X_val):\n","            if(np.array_equal(train_cube,val_cube)):\n","                print(f\"Copy found at (train,val) {i,j}\")\n","    #check for duplicates in train and test set\n","    for i,train_cube in enumerate(X_train):\n","        for j,test_cube in enumerate(X_test):\n","            if(np.array_equal(train_cube,test_cube)):\n","                print(f\"Copy found at (train,test) {i,j}\")\n","    #check for duplicates in validation and test set\n","    for i,val_cube in enumerate(X_val):\n","        for j,test_cube in enumerate(X_test):\n","            if(np.array_equal(val_cube,test_cube)):\n","                print(f\"Copy found at (val,test) {i,j}\")\n","    #check for duplicates in train set\n","    for i,train_cube in enumerate(X_train):\n","        for j,train_cube2 in enumerate(X_train):\n","            if(np.array_equal(train_cube,train_cube2) and i!=j):\n","                print(f\"Copy found in Train set {i,j}\")\n","    #check for duplicates in validation set\n","    for i,val_cube in enumerate(X_val):\n","        for j,val_cube2 in enumerate(X_val):\n","            if(np.array_equal(val_cube,val_cube2) and i!=j):\n","                print(f\"Copy found in Validation set {i,j}\")\n","    #check for duplicates in test set\n","    for i,test_cube in enumerate(X_test):\n","        for j,test_cube2 in enumerate(X_test):\n","            if(np.array_equal(test_cube,test_cube2) and i!=j):\n","                print(f\"Copy found in Test set {i,j}\")                "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# check for data leaks \n","check_data_leaks(X_train, X_val, X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# apply preprocessing\n","X_train = preprocess_cubes(X_train)\n","X_val = preprocess_cubes(X_val)\n","X_test = preprocess_cubes(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# delete variable to save RAM space\n","del patient_instances"]},{"cell_type":"markdown","metadata":{},"source":["# Data Visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# plot sample cube\n","def plot_sampled_cube(cube_set: np.ndarray, edge_size: int, label_set = None, index = None):\n","    \"\"\"\n","    Plot a sampled cube from a given cube set.\n","\n","    Parameters:\n","        cube_set (np.ndarray): The set of cubes to choose from.\n","        edge_size (int): The edge size of the cube.\n","        label_set (Optional[np.ndarray]): The corresponding labels (optional).\n","        index (Optional[int]): The index of the cube to plot. If not provided, a random cube is selected.\n","\n","    Returns:\n","        None: Displays the 3D scatter plot.\n","    \"\"\"\n","    # random selection\n","    if(index is None):\n","        index = np.random.choice(range(len(cube_set)))\n","    # plot\n","    fig = plt.figure()\n","    ax = fig.add_subplot(111, projection='3d')\n","    # get the indices for each dimension\n","    x, y, z = np.meshgrid(np.arange(edge_size), np.arange(edge_size), np.arange(edge_size))\n","    # flatten the arrays for plotting\n","    x = x.flatten()\n","    y = y.flatten()\n","    z = z.flatten()\n","    values = cube_set[index].flatten()\n","\n","    # Scatter plot\n","    ax.scatter(x, y, z, c=values, cmap='gray')\n","    # show label if required\n","    if (label_set is not None):\n","        ax.set_title(f\"label: {label_set[index]}\")\n","\n","    # Show the plot\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# visualize some cubes from each set\n","plot_sampled_cube(X_train, edge_size=24, label_set=y_train)\n","plot_sampled_cube(X_val, edge_size=24, label_set=y_val)\n","plot_sampled_cube(X_test, edge_size=24, label_set=y_test)"]},{"cell_type":"markdown","metadata":{},"source":["# Convolutional Neural Network"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# import tensorflow framework\n","import tensorflow as tf\n","import tensorflow.keras as tfk\n","import tensorflow.keras.layers as tfkl"]},{"cell_type":"markdown","metadata":{},"source":["## FastConv3DNet V1 Architecture"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# build the neural network layer by layer\n","def build_FastConv3DNet_V1(input_shape: tuple):\n","    \"\"\"\n","    Build a 3D convolutional neural network model for binary classification.\n","\n","    Parameters:\n","        input_shape (tuple): The input shape of the 3D data (e.g., (24, 24, 24, 1)).\n","\n","    Returns:\n","        tfk.Model: Compiled CNN model for binary classification.\n","    \"\"\"\n","    # input layer\n","    input_layer = tfkl.Input(shape=input_shape, name='Input')\n","    # first convolutional block\n","    x = tfkl.Conv3D(filters=8, kernel_size=3, strides=(1,1,1), padding=\"valid\", dilation_rate=(1,1,1))(input_layer)\n","    x = tfkl.BatchNormalization()(x)\n","    x = tfkl.ReLU()(x)\n","    # second convolutional block\n","    x = tfkl.Conv3D(filters=16, kernel_size=3, strides=(1,1,1), padding=\"valid\", dilation_rate=(1,1,1))(x)\n","    x = tfkl.BatchNormalization()(x)\n","    x = tfkl.ReLU()(x)\n","    # third convolutional block\n","    x = tfkl.Conv3D(filters=16, kernel_size=3, strides=(1,1,1), padding=\"valid\", dilation_rate=(2,2,2))(x)\n","    x = tfkl.BatchNormalization()(x)\n","    x = tfkl.ReLU()(x)\n","    # fourth convolutional block\n","    x = tfkl.Conv3D(filters=32, kernel_size=3, strides=(1,1,1), padding=\"valid\", dilation_rate=(4,4,4))(x)\n","    x = tfkl.BatchNormalization()(x)\n","    x = tfkl.ReLU()(x)\n","    # GAP layer\n","    x = tfkl.GlobalAveragePooling3D(name='GAP')(x)\n","    # Dense layer\n","    x = tfkl.Dense(units=32, activation='relu', name='dense')(x)\n","    x = tfkl.Dropout(0.5)(x)\n","    # Output layer\n","    output_layer = tfkl.Dense(units=1, activation='sigmoid', name='Output')(x)\n","    \n","    # connect input and output through the Model class\n","    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='FastConv3DNet_V1')\n","    # compile the model\n","    model.compile(loss=tfk.losses.BinaryCrossentropy(), optimizer=tfk.optimizers.Adam(1e-3), \n","                  metrics=['accuracy',tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n","     \n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# build FastConv3DNet_V1 model\n","model_fastconv3dnet_v1 = build_FastConv3DNet_V1(input_shape=(24,24,24,1))\n","model_fastconv3dnet_v1.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# visualize the model\n","tf.keras.utils.plot_model(model_fastconv3dnet_v1, show_shapes=True, show_layer_names=True)"]},{"cell_type":"markdown","metadata":{},"source":["## Squeeze and Excitation Block"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# define the squeeze and excitation block\n","def se_block(input_tensor, ratio=16):\n","    \"\"\"\n","    Creates a squeeze and excitation block.\n","\n","    Parameters:\n","    input_tensor (tensor): The input tensor to the block.\n","    ratio (int): The ratio for dimensionality reduction in the squeeze step.\n","\n","    Returns:\n","    output_tensor (tensor): The output tensor after the squeeze and excitation.\n","    \"\"\"\n","    init = input_tensor\n","    channel_axis = -1\n","    filters = init.shape[channel_axis]\n","    se_shape = (1, 1, 1, filters)\n","\n","    se = tfkl.GlobalAveragePooling3D()(init)\n","    se = tfkl.Reshape(se_shape)(se)\n","    se = tfkl.Dense(filters // ratio, activation='relu')(se)\n","    se = tfkl.Dense(filters, activation='sigmoid')(se)\n","\n","    x = tfkl.multiply([init, se])\n","    return x"]},{"cell_type":"markdown","metadata":{},"source":["## FastConv3DNet V2 Architecture"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# build the neural network layer by layer\n","def build_FastConv3DNet_V2(input_shape: tuple):\n","    \"\"\"\n","    Build a 3D convolutional neural network model for binary classification.\n","\n","    Parameters:\n","        input_shape (tuple): The input shape of the 3D data (e.g., (24, 24, 24, 1)).\n","\n","    Returns:\n","        tfk.Model: Compiled CNN model for binary classification.\n","    \"\"\"\n","    # input layer\n","    input_layer = tfkl.Input(shape=input_shape, name='Input')\n","    # first convolutional block\n","    x = tfkl.Conv3D(filters=8, kernel_size=3, strides=(1,1,1), padding=\"valid\", dilation_rate=(1,1,1))(input_layer)\n","    x = tfkl.BatchNormalization()(x)\n","    x = tfkl.ReLU()(x)\n","    # squeeze and excitation block\n","    x = se_block(x, ratio=2)\n","    # second convolutional block\n","    x = tfkl.Conv3D(filters=8, kernel_size=3, strides=(1,1,1), padding=\"valid\", dilation_rate=(1,1,1))(x)\n","    x = tfkl.BatchNormalization()(x)\n","    x = tfkl.ReLU()(x)\n","    # squeeze and excitation block\n","    x = se_block(x, ratio=2)\n","    # third convolutional block\n","    x = tfkl.Conv3D(filters=16, kernel_size=3, strides=(1,1,1), padding=\"valid\", dilation_rate=(2,2,2))(x)\n","    x = tfkl.BatchNormalization()(x)\n","    x = tfkl.ReLU()(x)\n","    # squeeze and excitation block\n","    x = se_block(x, ratio=4)\n","    # fourth convolutional block\n","    x = tfkl.Conv3D(filters=32, kernel_size=3, strides=(1,1,1), padding=\"valid\", dilation_rate=(4,4,4))(x)\n","    x = tfkl.BatchNormalization()(x)\n","    x = tfkl.ReLU()(x)\n","    # squeeze and excitation block\n","    x = se_block(x, ratio=8)\n","    # GAP layer\n","    x = tfkl.GlobalAveragePooling3D()(x)\n","    # Dense layer\n","    x = tfkl.Dense(units=32, activation='relu')(x)\n","    x = tfkl.Dropout(0.5)(x)\n","    # Output layer\n","    output_layer = tfkl.Dense(units=1, activation='sigmoid', name='Output')(x)\n","    \n","    # connect input and output through the Model class\n","    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='FastConv3DNet_V2')\n","    # compile the model\n","    model.compile(loss=tfk.losses.BinaryCrossentropy(), optimizer=tfk.optimizers.Adam(1e-3), \n","                  metrics=['accuracy',tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n","     \n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# build FastConv3DNet_V2 model\n","model_fastconv3dnet_v2 = build_FastConv3DNet_V2(input_shape=(24,24,24,1))\n","model_fastconv3dnet_v2.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# visualize the model\n","tf.keras.utils.plot_model(model_fastconv3dnet_v2, show_shapes=True, show_layer_names=True)"]},{"cell_type":"markdown","metadata":{},"source":["## Depthwise Separable Convolution"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# define DepthwiseSeparableConv3D layer \n","class DepthwiseSeparableConv3D(tf.keras.layers.Layer):\n","    def __init__(self, kernel_size, strides, padding, dilation_rate, **kwargs):\n","        super().__init__(**kwargs)\n","        self.kernel_size = kernel_size\n","        self.strides = strides\n","        self.padding = padding\n","        self.dilation_rate = dilation_rate\n","\n","    def build(self, input_shape):\n","        self.depthwise_conv = tf.keras.layers.Conv3D(\n","            filters=input_shape[-1],\n","            kernel_size=self.kernel_size,\n","            strides=self.strides,\n","            padding=self.padding,\n","            dilation_rate=self.dilation_rate,\n","            groups=input_shape[-1],  # Depthwise convolution\n","            use_bias=False\n","        )\n","        self.pointwise_conv = tf.keras.layers.Conv3D(\n","            filters=input_shape[-1],\n","            kernel_size=(1, 1, 1),\n","            strides=(1, 1, 1),\n","            padding=self.padding,\n","            use_bias=False\n","        )\n","        super().build(input_shape)\n","\n","    def call(self, inputs):\n","        x = self.depthwise_conv(inputs)\n","        return self.pointwise_conv(x)"]},{"cell_type":"markdown","metadata":{},"source":["## FasterConv3DNet V1 Architecture"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# build the neural network layer by layer\n","def build_FasterConv3DNet_V1(input_shape: tuple):\n","    \"\"\"\n","    Build a 3D convolutional neural network model for binary classification.\n","\n","    Parameters:\n","        input_shape (tuple): The input shape of the 3D data (e.g., (24, 24, 24, 1)).\n","\n","    Returns:\n","        tfk.Model: Compiled CNN model for binary classification.\n","    \"\"\"\n","    model = tf.keras.models.Sequential([\n","        # input layer\n","        tfkl.Input(shape=input_shape, name='Input'),\n","        # first convolutional block\n","        tfkl.Conv3D(filters=4, kernel_size=3, strides=(1,1,1), padding=\"valid\", dilation_rate=(1,1,1)),\n","        tfkl.BatchNormalization(),\n","        tfkl.ReLU(),\n","        # second convolutional block\n","        DepthwiseSeparableConv3D(kernel_size=3, strides=(1,1,1), padding=\"valid\", dilation_rate=(1,1,1)),\n","        tfkl.BatchNormalization(),\n","        tfkl.ReLU(),\n","        # third convolutional block\n","        DepthwiseSeparableConv3D(kernel_size=3, strides=(1,1,1), padding=\"valid\", dilation_rate=(2,2,2)),\n","        tfkl.BatchNormalization(),\n","        tfkl.ReLU(),\n","        # fourth convolutional block\n","        DepthwiseSeparableConv3D(kernel_size=3, strides=(1,1,1), padding=\"valid\", dilation_rate=(4,4,4)),\n","        tfkl.BatchNormalization(),\n","        tfkl.ReLU(),\n","        # GAP layer\n","        tfkl.GlobalAveragePooling3D(name='GAP'),\n","        # Dense layer\n","        tfkl.Dense(units=2, activation='relu', name='dense'),\n","        #tfkl.Dropout(0.5),\n","        # Output layer\n","        tfkl.Dense(units=1, activation='sigmoid', name='Output')\n","        ], name=\"FasterConv3DNet_V1\")\n","    \n","    # compile the model\n","    model.compile(loss=tfk.losses.BinaryCrossentropy(), optimizer=tfk.optimizers.Adam(1e-3), \n","                  metrics=['accuracy',tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n","     \n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# build FasterConv3DNet_V1 model\n","model_fasterconv3dnet_v1 = build_FasterConv3DNet_V1(input_shape=(24,24,24,1))\n","model_fasterconv3dnet_v1.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# visualize the model\n","tf.keras.utils.plot_model(model_fasterconv3dnet_v1, show_shapes=True, show_layer_names=True)"]},{"cell_type":"markdown","metadata":{},"source":["## FasterConv3DNet V2 Architecture"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# redefine the squeeze and excitation block as class\n","class SEBlock(tfkl.Layer):\n","    def __init__(self, ratio=2, **kwargs):\n","        super(SEBlock, self).__init__(**kwargs)\n","        self.ratio = ratio\n","\n","    def build(self, input_shape):\n","        filters = input_shape[-1]\n","        self.squeeze = tfkl.GlobalAveragePooling3D()\n","        self.excitation = tf.keras.Sequential([\n","            tfkl.Dense(filters // self.ratio, activation='relu'),\n","            tfkl.Dense(filters, activation='sigmoid')\n","        ])\n","\n","    def call(self, inputs):\n","        x = self.squeeze(inputs)\n","        x = tf.expand_dims(tf.expand_dims(tf.expand_dims(x, axis=1), axis=1), axis=1)\n","        scale = self.excitation(x)\n","        return inputs * scale\n","\n","# build the neural network layer by layer\n","def build_FasterConv3DNet_V2(input_shape: tuple):\n","    \"\"\"\n","    Build a 3D convolutional neural network model for binary classification.\n","\n","    Parameters:\n","        input_shape (tuple): The input shape of the 3D data (e.g., (24, 24, 24, 1)).\n","\n","    Returns:\n","        tfk.Model: Compiled CNN model for binary classification.\n","    \"\"\"\n","    \n","    model = tf.keras.models.Sequential([\n","        # input layer\n","        tfkl.Input(shape=input_shape, name='Input'),\n","        # first convolutional block\n","        tfkl.Conv3D(filters=4, kernel_size=3, strides=(1,1,1), padding=\"valid\", dilation_rate=(1,1,1)),\n","        tfkl.BatchNormalization(),\n","        tfkl.ReLU(),\n","        # squeeze and excitation block\n","        SEBlock(ratio=2),\n","        # second convolutional block\n","        DepthwiseSeparableConv3D(kernel_size=3, strides=(1,1,1), padding=\"valid\", dilation_rate=(1,1,1)),\n","        tfkl.BatchNormalization(),\n","        tfkl.ReLU(),\n","        # squeeze and excitation block\n","        SEBlock(ratio=2),\n","        # third convolutional block\n","        DepthwiseSeparableConv3D(kernel_size=3, strides=(1,1,1), padding=\"valid\", dilation_rate=(2,2,2)),\n","        tfkl.BatchNormalization(),\n","        tfkl.ReLU(),\n","        # squeeze and excitation block\n","        SEBlock(ratio=2),\n","        # fourth convolutional block\n","        DepthwiseSeparableConv3D(kernel_size=3, strides=(1,1,1), padding=\"valid\", dilation_rate=(4,4,4)),\n","        tfkl.BatchNormalization(),\n","        tfkl.ReLU(),\n","        # squeeze and excitation block\n","        SEBlock(ratio=2),\n","        # GAP layer\n","        tfkl.GlobalAveragePooling3D(name='GAP'),\n","        # Dense layer\n","        tfkl.Dense(units=2, activation='relu', name='dense'),\n","        # Output layer\n","        tfkl.Dense(units=1, activation='sigmoid', name='Output')\n","        ], name=\"FasterConv3DNet_V2\")\n","    \n","    # compile the model\n","    model.compile(loss=tfk.losses.BinaryCrossentropy(), optimizer=tfk.optimizers.Adam(1e-3), \n","                  metrics=['accuracy',tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n","     \n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# build FastConv3DNet_V1 model\n","model_fasterconv3dnet_v2 = build_FasterConv3DNet_V2(input_shape=(24,24,24,1))\n","model_fasterconv3dnet_v2.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# visualize the model\n","tf.keras.utils.plot_model(model_fasterconv3dnet_v2, show_shapes=True, show_layer_names=True)"]},{"cell_type":"markdown","metadata":{},"source":["## Model Training"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# create callbacks\n","\n","# early stopping \n","early_stopping = tf.keras.callbacks.EarlyStopping(\n","    monitor='val_accuracy',\n","    patience=20,\n","    mode='max',\n","    restore_best_weights=True,\n",")\n","\n","# learning rate scheduler\n","lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n","    monitor='val_accuracy',\n","    factor=0.9,\n","    patience=5,\n","    verbose=0,\n","    mode='max',\n","    min_lr=1e-6,\n",")\n","\n","callbacks = [early_stopping, lr_scheduler]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Train the models\n","\n","epochs = 200\n","batch_size = 64\n","\n","models = [model_fastconv3dnet_v1, model_fastconv3dnet_v2, model_fasterconv3dnet_v1, model_fasterconv3dnet_v2]\n","model_histories = []\n","\n","for model in models:\n","    print(f\"-> Training {model.name}\")\n","    model_history = model.fit(\n","        x = X_train,\n","        y= y_train,\n","        batch_size=batch_size,\n","        validation_data=(X_val,y_val),\n","        epochs=epochs, \n","        callbacks=callbacks,\n","    )\n","    model_histories.append(model_history)"]},{"cell_type":"markdown","metadata":{},"source":["## Model Assessment"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# show leanirng curves\n","def plot_learning_curves(model_history: tf.keras.callbacks.History):\n","    \"\"\"\n","    Plot learning curves for accuracy, loss, and learning rate.\n","\n","    Parameters:\n","        model_history (tf.keras.callbacks.History): History object obtained during model training.\n","    \"\"\"\n","    best_epoch = np.argmin(model_history.history['val_loss'])\n","    # show accuracy curve\n","    plt.figure(figsize=(20,5))\n","    plt.plot(model_history.history['accuracy'], label='Accuracy [train]', alpha=.8, color='#ff7f0e')\n","    plt.plot(model_history.history['val_accuracy'], label='Accuracy [val]', alpha=.9, color='#5a9aa5')\n","    plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n","    plt.title('Accuracy')\n","    plt.xlabel('epoch')\n","    plt.ylabel('Accuracy')\n","    plt.legend()\n","    plt.grid(alpha=.3)\n","    # show loss curve\n","    plt.figure(figsize=(20,5))\n","    plt.plot(model_history.history['loss'], label='Training Loss', alpha=.8, color='#ff7f0e')\n","    plt.plot(model_history.history['val_loss'], label='Validation Loss', alpha=.9, color='#5a9aa5')\n","    plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n","    plt.title('Loss')\n","    plt.xlabel('epoch')\n","    plt.ylabel('Cross Entropy')\n","    plt.legend()\n","    plt.grid(alpha=.3)\n","    # show learning rate curve\n","    plt.figure(figsize=(18,3))\n","    plt.plot(model_history.history['lr'], label='Learning Rate', alpha=.8, color='#ff7f0e')\n","    plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n","    plt.title('Learning Rate')\n","    plt.xlabel('epoch')\n","    plt.ylabel('Learning rate')\n","    plt.legend()\n","    plt.grid(alpha=.3)\n","\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# visualize learning history\n","for model_history, model in zip(model_histories, models):\n","    print(f\"-> Learning curves for {model.name}\")\n","    plot_learning_curves(model_history)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# define utilities to assess model performance\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n","import seaborn as sns\n","\n","def plot_confusion_matrix(true: np.ndarray, predictions: np.ndarray, output_activation: str):\n","    \"\"\"\n","    Plot the confusion matrix.\n","\n","    Parameters:\n","        true (np.ndarray): True labels.\n","        predictions (np.ndarray): Predicted labels.\n","        output_activation (str): Activation function used in the output layer of the model.\n","    \"\"\"\n","    # check it predictinos are returned as sigmoid output or softmax output\n","    if(output_activation==\"sigmoid\"):\n","        for i,pred in enumerate(predictions):\n","            predictions[i] = 0 if (pred<0.5) else 1\n","    elif(output_activation==\"softmax\"):\n","        true = np.argmax(true, axis=-1)\n","        predictions = np.argmax(predictions, axis=-1)\n","    \n","    # compute confusion matrix\n","    cm = confusion_matrix(true, predictions)\n","    \n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', cbar=False)\n","\n","    # add labels and title\n","    plt.xlabel('Predicted')\n","    plt.ylabel('True')\n","    plt.title('Confusion Matrix')\n","\n","    # show the plot\n","    plt.show()\n","    \n","def get_metrics(true: np.ndarray, predictions: np.ndarray, output_activation: str):\n","    \"\"\"\n","    Display classification metrics.\n","\n","    Parameters:\n","        true (np.ndarray): True labels.\n","        predictions (np.ndarray): Predicted labels.\n","        output_activation (str): Activation function used in the output layer of the model.\n","    \"\"\"\n","    # display the shape of the predictions\n","    print(\"Predictions Shape:\", predictions.shape)\n","    \n","    # check if predictions are returned as sigmoid output or softmax output\n","    if(output_activation==\"sigmoid\"):\n","        for i,pred in enumerate(predictions):\n","            predictions[i] = 0 if (pred<0.5) else 1\n","    elif(output_activation==\"softmax\"):\n","        true = np.argmax(true, axis=-1)\n","        predictions = np.argmax(predictions, axis=-1)\n","\n","    # Compute classification metrics\n","    accuracy = accuracy_score(true, predictions)\n","    precision = precision_score(true, predictions) \n","    recall = recall_score(true, predictions)\n","    f1 = f1_score(true, predictions)\n","\n","    # Display the computed metrics\n","    print('Accuracy:', accuracy.round(4))\n","    print('Precision:', precision.round(4))\n","    print('Recall:', recall.round(4))\n","    print('F1:', f1.round(4))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# predict labels for the entire test set\n","# estimate the time required for inference\n","import time \n","output_activation = \"sigmoid\"\n","for model in models:\n","    print(f\"-> Model: {model.name}\")\n","    times = []\n","    for i in range(100):\n","        start_time = time.time()\n","        predictions = model.predict(X_test, verbose=0)\n","        stop_time = time.time()\n","        times.append(stop_time - start_time)\n","    print(f\"Average inference time for {len(X_test)} samples: {np.mean(times)} seconds\")\n","    # get performance metrics \n","    get_metrics(y_test, predictions, output_activation)\n","    plot_confusion_matrix(y_test, predictions, output_activation)"]},{"cell_type":"markdown","metadata":{},"source":["## Prediction Visualization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# modify get_labels to return also node ids\n","def get_labeled_patches_new(\n","    image: HearticDatasetManager.cat08.Cat08ImageCT|HearticDatasetManager.asoca.AsocaImageCT,\n","    graph: hcatnetwork.graph.graph.SimpleCenterlineGraph,\n","    rotation_random_sampling = None,\n","    rotation_local_sampling = None\n","):\n","    \n","    \"\"\" \n","    The function takes as input a single image-graph pair and provides the extracted patches and labels by following these steps:\n","    \n","        1. Firstly it samples a point along the graph and applies a random translation to it.\n","        2. Then utility funcitons are used to sample a cube centered in such point.\n","        3. If the point lies close enough to one of the two Ostia, then label = 1, 0 otherwise.\n","        4. Finally, steps 1-2 are repeated by local sampling around the Ostium to ensure class 1 is represented. \n","    \n","    Parameters:\n","        image (HearticDatasetManager.cat08.Cat08ImageCT | HearticDatasetManager.asoca.AsocaImageCT):\n","            The CT scan image data.\n","        graph (hcatnetwork.graph.graph.SimpleCenterlineGraph):\n","            The corresponding centerline graph.\n","        rotation_random_sampling (optional): Rotation parameters for random sampling. Default is None.\n","        rotation_local_sampling (optional): Rotation parameters for local sampling. Default is None.\n","\n","    Returns:\n","        tuple: A tuple containing numpy arrays of extracted cubes, corresponding labels and nodes.\n","    \"\"\"\n","    \n","    # compute radius statistics\n","    maxR, minR, avgR = compute_r_stats(graph)\n","    # retrieve ostia\n","    ids, coord_ostium_1, coord_ostium_2 = get_ostia_data(graph)\n","    \n","    cubes = []\n","    labels = []\n","    nodes = []\n","    \n","\n","    # firstly random sample along the graph\n","    for cube_i in tqdm(range(NUM_CUBES_PATIENT // 2), desc=\"Random Sampling\"):\n","        # choose a random node of the graph\n","        node_id = np.random.choice(list(graph.nodes.keys()))\n","        # get the position of the node in RAS\n","        node_position = np.array([graph.nodes[node_id][\"x\"], graph.nodes[node_id][\"y\"], graph.nodes[node_id][\"z\"]])\n","        nodes.append(node_position) # save the node position\n","        # select parameters for random translation\n","        r = np.random.uniform(-avgR*1.5, avgR*1.5)#translation vector\n","        theta = np.random.uniform(0, 2*np.pi)#xy plane angle\n","        phi = np.random.uniform(0, np.pi)#z to xy plane angle\n","        # apply the translation to the selected point\n","        node_position += np.array([r*np.sin(phi)*np.cos(theta), r*np.sin(phi)*np.sin(theta), r*np.cos(phi)]).reshape(3,1)\n","        if (rotation_random_sampling):\n","            # define rotation vector\n","            vector_axis_of_rotation = np.array([np.random.uniform(-1, 1) , np.random.uniform(-1, 1), np.random.uniform(-1, 1)])\n","            transformation_to_apply = HearticDatasetManager.affine.get_affine_3d_rotation_around_vector(\n","                vector=vector_axis_of_rotation,\n","                vector_source=node_position.reshape(3,1), # the center of the cube\n","                rotation=np.random.choice(range(90+1)), # max degree of rotation +1 \n","                rotation_units=\"deg\"\n","            )\n","        else:\n","            transformation_to_apply = None\n","        # sample\n","        cube_array = get_input_data_from_vertex_ras_position(image,node_position,\n","                                                             CUBE_SIDE_MM,CUBE_SIDE_N_SAMPLES, \n","                                                             affine=transformation_to_apply)\n","        # compute distances from ostia to assign labels\n","        dist1 = np.linalg.norm(coord_ostium_1[:3] - node_position)\n","        dist2 = np.linalg.norm(coord_ostium_2[:3] - node_position)\n","        label = 1 if (dist1<=1.2*coord_ostium_1[-1] or dist2<=1.2*coord_ostium_2[-1]) else 0\n","\n","        cubes.append(cube_array)\n","        labels.append(label)\n","        # Simulating loading time for each iteration\n","        time.sleep(0.1)\n","        \n","    # then sample locally from the ostia applying augmentation via rotation if selected\n","    for cube_i in tqdm(range(NUM_CUBES_PATIENT // 2), desc=\"Local Sampling\"):\n","        # select one of the two ostia\n","        node_id = np.random.choice(ids)\n","        # get the position of the node in RAS\n","        node_position = np.array([graph.nodes[node_id][\"x\"], graph.nodes[node_id][\"y\"], graph.nodes[node_id][\"z\"]])\n","        nodes.append(node_position) # save the node position\n","        # select parameters for random translation\n","        r = np.random.uniform(-graph.nodes[node_id][\"r\"]*1.2, graph.nodes[node_id][\"r\"]*1.2)#translation vector\n","        theta = np.random.uniform(0, 2*np.pi)#xy plane angle\n","        phi = np.random.uniform(0, np.pi)#z to xy plane angle\n","        # apply the translation to the selected point\n","        node_position += np.array([r*np.sin(phi)*np.cos(theta), r*np.sin(phi)*np.sin(theta), r*np.cos(phi)]).reshape(3,1)\n","        if (rotation_local_sampling):\n","            # define rotation vector\n","            vector_axis_of_rotation = np.array([np.random.uniform(-1, 1) , np.random.uniform(-1, 1), np.random.uniform(-1, 1)])\n","            transformation_to_apply = HearticDatasetManager.affine.get_affine_3d_rotation_around_vector(\n","                vector=vector_axis_of_rotation,\n","                vector_source=node_position.reshape(3,1), # the center of the cube\n","                rotation=np.random.choice(range(90+1)), # max degree of rotation +1 \n","                rotation_units=\"deg\"\n","            )\n","        else:\n","            transformation_to_apply = None\n","        # sample\n","        cube_array = get_input_data_from_vertex_ras_position(image,node_position,\n","                                                             CUBE_SIDE_MM,CUBE_SIDE_N_SAMPLES, \n","                                                             affine=transformation_to_apply)\n","\n","        cubes.append(cube_array)\n","        labels.append(1)\n","\n","        # simulating loading time for each iteration\n","        time.sleep(0.1)\n","    \n","    \n","    cubes = np.array(cubes)\n","    labels = np.array(labels)\n","    nodes = np.array(nodes)\n","        \n","    return cubes, labels, nodes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# visualize predictions for patient 6 in CAT08 dataset\n","patient_id = 6\n","# define an empty patient instance\n","patient6 = Patient()\n","# download patient image and graph\n","image_file = os.path.join(CAT08_FOLDER, HearticDatasetManager.cat08.DATASET_CAT08_IMAGES[patient_id])\n","\n","graph_file = os.path.join(CAT08_FOLDER, HearticDatasetManager.cat08.DATASET_CAT08_GRAPHS_RESAMPLED_05MM[patient_id])\n","\n","image = HearticDatasetManager.cat08.Cat08ImageCT(image_file)\n","graph = hcatnetwork.io.load_graph(graph_file, output_type=hcatnetwork.graph.SimpleCenterlineGraph)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# resample the graph\n","resampled_graph = graph.resample(mm_between_nodes=0.1)\n","hcatnetwork.draw.draw_simple_centerlines_graph_2d(resampled_graph)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# save Ostia coordinates and convert to RAS\n","ids = resampled_graph.get_coronary_ostia_node_id()\n","coord_ostium_1 = np.zeros(4)\n","coord_ostium_2 = np.zeros(4)\n","\n","coord_ostium_1[0]= graph.nodes[ids[0]]['x']\n","coord_ostium_1[1]= graph.nodes[ids[0]]['y']\n","coord_ostium_1[2]= graph.nodes[ids[0]]['z']\n","coord_ostium_1[3]= graph.nodes[ids[0]]['r']\n","coord_ostium_2[0]= graph.nodes[ids[1]]['x']\n","coord_ostium_2[1]= graph.nodes[ids[1]]['y']\n","coord_ostium_2[2]= graph.nodes[ids[1]]['z']\n","coord_ostium_2[3]= graph.nodes[ids[1]]['r']\n","\n","coord_ostium_1_ras = HearticDatasetManager.affine.apply_affine_3d(image.affine_centerlines2ras, coord_ostium_1[0:3])\n","coord_ostium_2_ras = HearticDatasetManager.affine.apply_affine_3d(image.affine_centerlines2ras, coord_ostium_2[0:3])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# convert all graph coordinates to RAS\n","for node_id in resampled_graph.nodes:\n","  old_coords = np.array([resampled_graph.nodes[node_id][\"x\"], resampled_graph.nodes[node_id][\"y\"], resampled_graph.nodes[node_id][\"z\"]])\n","  new_coords = HearticDatasetManager.affine.apply_affine_3d(image.affine_centerlines2ras, old_coords)\n","  resampled_graph.nodes[node_id][\"x\"] = new_coords[0]\n","  resampled_graph.nodes[node_id][\"y\"] = new_coords[1]\n","  resampled_graph.nodes[node_id][\"z\"] = new_coords[2]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# get distance from closest ostium for each node\n","sorted_nodes = []\n","for node_id in resampled_graph.nodes():\n","  node_position = np.array([resampled_graph.nodes[node_id][\"x\"], resampled_graph.nodes[node_id][\"y\"], resampled_graph.nodes[node_id][\"z\"]])\n","  dist1=((coord_ostium_1_ras[0]-node_position[0])**2 + (coord_ostium_1_ras[1]-node_position[1])**2 + (coord_ostium_1_ras[2]-node_position[2])**2)**(0.5)\n","  dist2=((coord_ostium_2_ras[0]-node_position[0])**2 + (coord_ostium_2_ras[1]-node_position[1])**2 + (coord_ostium_2_ras[2]-node_position[2])**2)**(0.5)\n","  if(dist1 > dist2):\n","    dist=dist2\n","  else:\n","    dist=dist1\n","  node_info = np.array([resampled_graph.nodes[node_id][\"x\"], resampled_graph.nodes[node_id][\"y\"], resampled_graph.nodes[node_id][\"z\"], dist])\n","  sorted_nodes.append(node_info)\n","\n","# sort the nodes based on distance from the ostia\n","sorted_nodes.sort(key=lambda x:x[-1])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# build the volumes\n","cubes = []\n","labels = []\n","for i in range(1500):\n","  node_position = np.array([sorted_nodes[i][0],sorted_nodes[i][1],sorted_nodes[i][2]])\n","  cube_array = get_input_data_from_vertex_ras_position(image,node_position,\n","                                                             CUBE_SIDE_MM,CUBE_SIDE_N_SAMPLES)\n","\n","  label = 1 if (sorted_nodes[i][-1]<=1.2*coord_ostium_1[-1] or sorted_nodes[i][-1]<=1.2*coord_ostium_2[-1]) else 0\n","  cubes.append(cube_array)\n","  labels.append(label)\n","\n","cubes = np.array(cubes)\n","labels = np.array(labels)\n","\n","# apply preprocessing\n","cubes_preprocessed = preprocess_cubes(cubes)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# get predictions\n","sorted_predictions = []\n","for model in models:\n","    sorted_prediction = model.predict(cubes_preprocessed)\n","    sorted_predictions.append(sorted_prediction)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","# plot the vessels and predicted labels\n","def plot_vessels(graph: hcatnetwork.graph.graph.SimpleCenterlineGraph, \n","                 image: HearticDatasetManager.cat08.Cat08ImageCT|HearticDatasetManager.asoca.AsocaImageCT, \n","                 coord_ostium_1_ras: np.ndarray, \n","                 coord_ostium_1: np.ndarray, \n","                 coord_ostium_2_ras: np.ndarray, \n","                 coord_ostium_2: np.ndarray, \n","                 sorted_predictions: List[int], \n","                 labels: List[int], \n","                 sorted_nodes: List[np.ndarray]):\n","    \"\"\"\n","    Parameters:\n","    - graph: NetworkX graph representing vessels and associated nodes.\n","    - image: Image data containing affine transformation information.\n","    - coord_ostium_1_ras, coord_ostium_1, coord_ostium_2_ras, coord_ostium_2: Coordinates for ostium points in RAS space.\n","    - sorted_predictions: Predicted labels for vessels, sorted for plotting.\n","    - labels: True labels for vessels.\n","    - sorted_nodes: Coordinates of vessels, sorted for plotting.\n","\n","    Returns:\n","    - mispredicted_nodes_indexes: List of indexes corresponding to mispredicted nodes for further analysis.\n","    \"\"\"\n","    # Extract vessel coordinates from the graph\n","    x_vessels = []\n","    y_vessels = []\n","    z_vessels = []\n","    for node_id in graph.nodes:\n","        coords = np.array([graph.nodes[node_id][\"x\"], graph.nodes[node_id][\"y\"], graph.nodes[node_id][\"z\"]])\n","        new_coords = HearticDatasetManager.affine.apply_affine_3d(image.affine_centerlines2ras, coords)\n","        x_vessels.append(new_coords[0])\n","        y_vessels.append(new_coords[1])\n","        z_vessels.append(new_coords[2])\n","\n","    # Create a 3D scatter plot\n","    fig = plt.figure(figsize=(10, 10))\n","    ax = fig.add_subplot(111, projection='3d')\n","\n","    # Plot vessels in blue\n","    ax.scatter(x_vessels, y_vessels, z_vessels, c='blue', marker='o', s=1)\n","\n","    # Plot ostium coordinates in green\n","    ax.scatter(coord_ostium_1_ras[0], coord_ostium_1[1], coord_ostium_1[2], c='green', marker='o', s=100)\n","    ax.scatter(coord_ostium_2_ras[0], coord_ostium_2[1], coord_ostium_2[2], c='green', marker='o', s=100)\n","\n","    # Iterate through sorted predictions and plot nodes with colors based on correctness\n","    mispredicted_nodes_indexes = []\n","    for i in range(len(sorted_predictions)):\n","        if labels[i] == sorted_predictions[i]:\n","            col = \"green\"\n","            lab = \"Correct labels\"\n","        else:\n","            col = \"red\"\n","            lab = \"Mispredicted labels\"\n","            mispredicted_nodes_indexes.append(i)\n","        ax.scatter(sorted_nodes[i][0], sorted_nodes[i][1], sorted_nodes[i][2], c=col, label=lab, marker='o', s=20)\n","\n","    # Set axis labels and plot title\n","    ax.set_xlabel('X-axis')\n","    ax.set_ylabel('Y-axis')\n","    ax.set_zlabel('Z-axis')\n","    ax.set_title('3D Scatter Plot of Patient 6 from the CAT08 dataset')\n","\n","    # Add legend for clarity\n","    ax.legend([\"Artery\", \"Mispredicted labels\", \"Correct labels\"], loc=\"best\")\n","\n","    # Display the plot\n","    plt.show()\n","\n","    # Return indexes of mispredicted nodes for further analysis\n","    return mispredicted_nodes_indexes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# compute the distance from the ostium of the mispredicted labels\n","mispredicted_nodes_indexes_dict = {}\n","for model in models:\n","    mispredicted_nodes_indexes = plot_vessels(graph, image, \n","                                              coord_ostium_1_ras, coord_ostium_1,\n","                                              coord_ostium_2_ras, coord_ostium_2, \n","                                              sorted_predictions, labels, sorted_nodes)\n","    mispredicted_nodes_indexes_dict[model] = mispredicted_nodes_indexes\n","\n","mispredicted_distances_dict = {}\n","for model, mispredicted_nodes_indexes in mispredicted_nodes_indexes_dict.items():\n","    mispredicted_distances = []\n","    for i in mispredicted_nodes_indexes:\n","        mispredicted_distances.append(np.around(sorted_nodes[i][3], decimals=1))\n","    mispredicted_distances_dict[model] = mispredicted_distances"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# plot the mispredictions histogram for each model\n","for model, mispredicted_distances in mispredicted_distances_dict.items():\n","    plt.hist(mispredicted_distances, bins='auto', range=(2.5, 4.5), align='left', edgecolor='black')\n","\n","    # Set labels and title\n","    plt.xlabel('Distance from the ostium [mm]')\n","    plt.ylabel('Mispredictions')\n","    plt.title(f'Frequency of mispredictions for {model}')\n","\n","    # Show the plot\n","    plt.show()"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4032015,"sourceId":7012634,"sourceType":"datasetVersion"},{"datasetId":4100692,"sourceId":7111704,"sourceType":"datasetVersion"}],"dockerImageVersionId":30587,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"}},"nbformat":4,"nbformat_minor":4}
